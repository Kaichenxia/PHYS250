%----------------------------------------------------
% Setup Beamer
%----------------------------------------------------
\documentclass[hyperref={colorlinks=true}]{beamer}

%----------------------------------------------------
% Packages to use
%----------------------------------------------------
\input{../packages.sty}

%----------------------------------------------------
% Setup Theme
%----------------------------------------------------
\input{../theme.sty}

%----------------------------------------------------
% Table of Contents at each section transition
%----------------------------------------------------

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \setcounter{tocdepth}{2}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------
% Colors
%----------------------------------------------------
\input{../mycolors.sty}

%----------------------------------------------------
% Style, formatting, and new commands
%----------------------------------------------------
\input{../../global.sty}
\input{../newcommands.sty}
\input{../EandMcommands.sty}

%----------------------------------------------------
% Set paths for plots and images
%----------------------------------------------------
\input{../paths.sty}

%----------------------------------------------------
% SETTINGS FOR THIS LECTURE
%----------------------------------------------------
\newcommand{\lecnum }  {Lecture 10}
\newcommand{\lecdate}  {November 6, 2018}
\newcommand{\topic}    {From Newton to Runge Kutta to ODEs!}

%-----------------------------------------------------------------------------------------
% Title: [Column]{Title}
%-----------------------------------------------------------------------------------------
\title[PHYS 250 (Autumn 2018) -- \lecnum]{\topic}

%-----------------------------------------------------------------------------------------
% SubTitle: [Column]{Subtitle}
%-----------------------------------------------------------------------------------------
\subtitle{PHYS 250 (Autumn 2018) -- \lecnum}

%-----------------------------------------------------------------------------------------
% Author: [SubAuthor]{Author}
%-----------------------------------------------------------------------------------------
\author[D.W.~Miller]{David Miller}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\institute[EFI, Chicago] 
{
  Department of Physics and the Enrico Fermi Institute\\
  University of Chicago
}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\date[\lecdate]{\lecdate}

\subject{PHYS 250 Lecture}

\begin{document}

%==========================================================================================
% TITLE PAGE
%==========================================================================================

{
\begin{frame}
  \titlepage
\end{frame}
}

%==========================================================================================
\section[Reminders]{Reminders}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Reminders from Lecture 9]{Reminders from Lecture 9}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Reminders from last time}

  Looked at two primary and exemplary methods for root finding, which is part of the foundation of optimization and differential equation solving.
  
  \vspace{0.3cm}
  
  \begin{ucblock}{Fundamental root finding methods}
    \begin{itemize}
      \item \bluebf{Bisection method (aka ``incremental search''):} 
      \begin{itemize}
        \item \textbf{PROs:} exceptionally simple and requires no knowledge of the function whose roots are sought
        \item \textbf{CONs:} doesn't use the potentially very useful knowledge of the roots that are sought
      \end{itemize}
      \item \bluebf{Newton's Method:} 
      \begin{itemize}
        \item \textbf{PROs:} converges much faster than bisection
        \item \textbf{CONs:} requires a calculation or estimation of the first derivative of the function
      \end{itemize}
    \end{itemize}
  \end{ucblock}
  
  Today, we will expand on these algorithms and go several steps further.

\end{frame}


%==========================================================================================
\section[Newton's method and related issues]{Newton's method and related issues}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Follow-up discussion]{Follow-up discussion}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Recall the description of Newton's method}

  Recall that Newton's method uses the Taylor expansion
  
  \begin{equation}
    F(x_0) = F(x+\delta) \approx F(x) + \delta F^{\prime}(x) + \frac{1}{2}\delta^2 F^{\prime\prime}(x) + \mathcal{O}(\delta^3)
  \end{equation}
  
  to inform the use of a linear approximation $\delta \approx \Delta$ where
  
  \begin{equation}
    \Delta = - \frac{F(x)}{F^{\prime}(x)}
  \end{equation}
  
  That gives way to an iterative approach that updates the estimate of the position of the root of $F(x)$ as being at $x_{i+1}$:
  
  \begin{equation}
    x_{i+1} = x_{i} - \frac{F(x_i)}{F^{\prime}(x_i)}
  \end{equation}
  
  The iteration stops after $j$ iterations when 
  
  \begin{equation}
   | x - x_j | \leq \epsilon
  \end{equation}  

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Precision of Newton's method}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      For any estimate $x_i$ of the method, the error, $E_i$ is the difference between the true root $x$ and the estimate:
      
      \begin{equation}
        E_i = x - x_i
      \end{equation}
   
      Merely by inspecting the design of Newton's method, you can see that the precision of the estimate for a subsequent iteration will be given by
      %
      \begin{eqnarray}
        E_{i+1} &=& E_i + \frac{F(x)}{F^{\prime}(x)} \\
                &=& -\frac{F^{\prime\prime}(x)}{2F^{\prime}(x)}E_i^2
      \end{eqnarray}
   
    \column{0.5\textwidth}
    
      \begin{figure}
        \centering
        \includegraphics[width=\columnwidth]{NewtonsMethodExample-lnx.png}
      \end{figure}
    
  \end{columns}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Convergence of Newton's method}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      Consequently, Newton's method \bluebf{converges quadratically}
      \begin{itemize}
        \item the error is the square of the error in the previous step)
        \item the number of significant figures is roughly doubled in every iteration, provided that $x_i$ is \alertbf{sufficiently} close to the root.
      \end{itemize} 
         
      However, a critical assumption is that $F^{\prime}(x) \neq 0$; for all $x \in I$, where $I$ is the interval $[x - r, x + r]$ for some $r \geq |x - x_0|$ and $x$ is the true root and $x_0$ was the starting point.
         
    \column{0.5\textwidth}
    
      \begin{figure}
        \includegraphics[width=\columnwidth]{NewtonsMethodExample-lnx.png}
      \end{figure}
    
  \end{columns}

\end{frame}


%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Pathologies and divergent scenarios}

  \setbeamercovered{transparent}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      That is definitely not always the case. Let's look at a pathological example. Here is a fun mystery function that I cooked up (since you need to do something similar on your homework):
      
      \begin{itemize}[<+->]
         \item $x_0 = 2.000$, \bluebf{7 iterations}
         \item $x_0 = -2.000$, \bluebf{2 iterations}
         \item $x_0 = 3.000$, \bluebf{2 iterations}
         \item $x_0 = -1.214$, \bluebf{4 iterations}
         \item Slight modification: $x_0 = -1.213$, \alertbf{no convergence}
         \item Slight modification: $x_0 = 3.000$, \alertbf{no convergence}
      \end{itemize}  
         
    \column{0.5\textwidth}
      
      \begin{figure}
        \centering
        \foreach \n in {1,...,6}%
          {\includegraphics<\n>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology\n.png}}
      \end{figure}
    
  \end{columns}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Backtracking}

  In the last examples above we have a case where the search falls into the pathology of a situation where the initial guess was not \alertbf{sufficiently close} to the root. an ``infinite'' loop without ever getting there. 
  
  \mysp
  
  A solution to this problem is called \alertbf{backtracking}. 
  
  \mysp
  
  \begin{ucblock}{Backtracking}
    In cases where the new guess $x_0 + \Delta x$ leads to an increase in the magnitude of the function, $|f(x_0 + \Delta x)|^{2} > |f(x_0)|^{2}$, you should backtrack somewhat and try a smaller guess, say, $x_0 + \Delta x/2$. If the magnitude of $f$ still increases, then you just need to backtrack some more, say, by trying $x_0 + \Delta x/4$ as your next guess, and so forth.
  \end{ucblock}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Pathological case fixed with backtracking}

  \setbeamercovered{transparent}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      Fixing the pathological example with backtracking:
      
      \begin{itemize}[<+->]
         \item $x_0 = -1.213$, \alertbf{no convergence}
         \item $x_0 = 3.000$, \alertbf{no convergence}
         \item $x_0 = 1.500$, \bluebf{3 iterations}
      \end{itemize}  
         
    \column{0.5\textwidth}
      
      \begin{figure}
        \centering
        \includegraphics<1>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology5.png}
        \includegraphics<2>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology6.png}
        \includegraphics<3>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology7.png}
      \end{figure}
    
  \end{columns}

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[System of equations]{System of equations}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Multidimensional problems}

  Up to this point, we have confined our attention to solving the single equation $F(x) = 0$. Let us now consider the $n$-dimensional version of the same problem, namely
  
  \begin{equation}
    \vec{F}(\vec{x}) = 0
  \end{equation}

  where we allow for a vector of functions $\vec{F} = \{f_1(\vec{x}), f_2(\vec{x}), ..., f_n(\vec{x})\}$, and $\vec{x} = \{ x_1, x_2, ..., x_{n} \}$.
  
  \mysp
  
  The solution of $n$ simultaneous, nonlinear equations is a much more formidable task than finding the root of a single equation. The trouble is the lack of a reliable method for bracketing the solution vector $\vec{x}$. Therefore, we cannot always provide the solution algorithm with a good starting value of $x$, unless such a value is suggested by the physics of the problem.
  
  \mysp
  
  Newton's method is the workhorse here!
  

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Reminder of the general problem}

  Start by considering each one of the $n$ functions, $f_n(x)$ separately:

  \begin{eqnarray}
    f_i(\vec{x}) &=& f_i(\vec{a} )+ \sum_j^n \frac{\partial f_i}{\partial x_j}|_{\vec{x}} \Delta x_j + \frac{1}{2!} \sum_j^n \sum_k^n \frac{\partial^2 f_i}{\partial_j \partial_k}|_{\vec{x}} \Delta x_j \Delta x_k \\
                 &=& f_i(\vec {a} )+ (\vec{x} - \vec{a})^{\mathrm {T} }  \nabla f(\vec{a}) + \frac{1}{2!}(\vec{x} - \vec{a})^{\mathrm {T} }\mathbf {H} (\vec{a})(\vec{x} - \vec{a})
  \end{eqnarray}
  
  where $\mathbf {H}$ is the \bluebf{Hessian matrix}, describing the \alertbf{curvature} of $f(\vec{x})$ by

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Jacobian matrix}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Examples}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Comments on matrix computing and manipulations}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}



%-----------------------------------------------------------------------------------------
\subsection[Numerical differentiation]{Numerical differentiation}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Finite difference approximation}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Derivatives by Interpolation}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%==========================================================================================
\section[Initial value problems in ODEs]{Initial value problems in ODEs}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Statement of the problem]{Statement of the problem}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Initial vs. boundary values}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Taylor series method]{Taylor series method}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Recall the Taylor series approach generally}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Runge-Kutta methods]{Runge-Kutta methods}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Avoiding repeated differentiation: Runge?Kutta}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Precision}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Higher order calculations: \texttt{rk4}}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%==========================================================================================
\section[Boundary value problems in ODEs]{Boundary value problems in ODEs}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Statement of the problem]{Statement of the problem}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{The issue}

  In an initial value problem we were able to start at the point where the initial values were given and march the solution forward as far as needed. This technique does not work for boundary value problems, because there are not enough starting conditions available at either endpoint to produce a unique solution.
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Shooting method]{Shooting method}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Recall the Taylor series approach generally}

  Something
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Something
    \begin{itemize}
      \item Something else 
    \end{itemize}
  \end{itemize}
  
  Lastly

\end{frame}


%==========================================================================================
%==========================================================================================
\end{document}
