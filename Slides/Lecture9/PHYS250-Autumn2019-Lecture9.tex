%----------------------------------------------------
% Setup Beamer
%----------------------------------------------------
\documentclass[hyperref={colorlinks=true}]{beamer}

%----------------------------------------------------
% Packages to use
%----------------------------------------------------
\input{../packages.sty}

%----------------------------------------------------
% Setup Theme
%----------------------------------------------------
\input{../theme.sty}

%----------------------------------------------------
% Table of Contents at each section transition
%----------------------------------------------------

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \setcounter{tocdepth}{2}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------
% Colors
%----------------------------------------------------
\input{../mycolors.sty}

%----------------------------------------------------
% Style, formatting, and new commands
%----------------------------------------------------
\input{../../global.sty}
\input{../newcommands.sty}
\input{../EandMcommands.sty}

%----------------------------------------------------
% Set paths for plots and images
%----------------------------------------------------
\input{../paths.sty}

%----------------------------------------------------
% SETTINGS FOR THIS LECTURE
%----------------------------------------------------
\newcommand{\lecnum }  {Lecture 9}
\newcommand{\lecdate}  {October 29, 2019}
\newcommand{\topic}    {Minimization, root finding, and solutions to ODEs}

%-----------------------------------------------------------------------------------------
% Title: [Column]{Title}
%-----------------------------------------------------------------------------------------
\title[PHYS 250 (Autumn 2019) -- \lecnum]{\topic}

%-----------------------------------------------------------------------------------------
% SubTitle: [Column]{Subtitle}
%-----------------------------------------------------------------------------------------
\subtitle{PHYS 250 (Autumn 2019) -- \lecnum}

%-----------------------------------------------------------------------------------------
% Author: [SubAuthor]{Author}
%-----------------------------------------------------------------------------------------
\author[D.W.~Miller]{David Miller}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\institute[EFI, Chicago] 
{
  Department of Physics and the Enrico Fermi Institute\\
  University of Chicago
}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\date[\lecdate]{\lecdate}

\subject{PHYS 250 Lecture}

\begin{document}

%==========================================================================================
% TITLE PAGE
%==========================================================================================

{
\begin{frame}
  \titlepage
\end{frame}
}

%==========================================================================================
\section[Connection between minimization and solving differential equations]{Connection between minimization and solving differential equations}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Reminders from Lecture 8]{Reminders from Lecture 8}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Reminders from last time}

  There are many examples of optimization in physics
  
  \vspace{0.3cm}
  
  \begin{ucblock}{Examples of minimization}
    \begin{itemize}
      \item \bluebf{Fitting a model to data:} minimize differences between a model and data
      \item \bluebf{Second law of thermodynamics:} minimize changes in entropy for a system in thermodynamic equilibrium
      \item \bluebf{Conservation of momentum:} establish mechanical equilibrium by minimizing changes in momentum, $\frac{d\vec{p}}{dt}=0$
      \item \bluebf{Principle of least action:} obtain the equations of motion of a system by minimizing (or maximizing!) the variations of the action, $S$
      \item \bluebf{Path integral formulation of quantum mechanics:} sort quantum mechanically possible trajectories by minimizing quantum action
      \item \bluebf{Ising model:} minimization the energy of the spin configurations
    \end{itemize}
  \end{ucblock}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Optimization and root finding}

  Ultimately, I want to bring us to the point of discussing \bluebf{optimization in the context of modeling} (c.f. Lecture 1!). 
  
  \vspace{0.4cm}
  
  What that means, is that we want to have the computational tools to be able to \bluebf{model data and approximate complex functions computationally}, since it is often \alertbf{impossible to do so analytically}.
  
  \vspace{0.4cm}
  
  To achieve that, let's expand our tool kit beyond the analytically solvable example from last time (the 2D linear regression for $\chi^2$ minimization).

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Categories of problem]{Categories of problem}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=20]
  \frametitle{Categorizing the problems}

  As we discussed in Lecture 8, an \bluebf{optimization problem} quickly turns into an attempt to solve a first-order ordinary (ODE) or partial differential equation (PDE) in order to identify the value of the parameter $\alpha$ for which the function $F$ is stationary:
  %
  \begin{equation}
    F^{\prime}(\alpha) = 0
  \end{equation}
  
  The approaches used to do this are closely related to the methods used for finding zeroes or \alertbf{``roots''} of functions (since the derivative of a function is, of course, also a function!). But we need to differentiate the problems:
  
  \begin{ucblock}{Initial vs. boundary value problems}
    Imagine you have the ODE
    %
    \begin{equation}
      F^{\prime\prime} = -F
    \end{equation}
    %
    \vspace{-0.5cm}
    %
    \begin{itemize}
      \item \bluebf{Initial value problem:}  $F(0)=1, F^{\prime}(0)=0$
      \item \bluebf{Boundary value problem:} $F(0)=1, F(\pi)=0$
    \end{itemize} 
    %
    The latter is often much harder!
  \end{ucblock}
  
  

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Specific example]{Specific example}
%-----------------------------------------------------------------------------------------

\begin{frame}[shrink=20]
  \frametitle{Explicit root finding: square well potential}

  \begin{columns}
  
  \column{0.50\textwidth}
    
  The Schrodinger equation  
  %
  \begin{equation}
    \frac{\hbar^2}{2m}\psi^{\prime\prime}(x) + V(x)\psi(x) = E\psi(x)
  \end{equation}
  
  \pause
  
  tells us that for a finite square-well potential
  
  \begin{figure}
    \includegraphics[width=0.6\columnwidth]{Finite_Square_Potential_Well.png}
  \end{figure}
  
  \vspace{-0.5cm}
  
  \begin{equation}
    V(x) = \begin{cases}  
             -V_0 & 0 < x < L \\
             0    & \mathrm{else}
           \end{cases}
  \end{equation}
  
  \pause
  
  and a bound state ($E=-f<0$, $f$ positive definite) in the region $x<0$, we must solve:
  
  \begin{equation}
    -\frac{\hbar^2}{2m}\psi^{\prime\prime}(x<0) = -f\psi(x<0)
  \end{equation}
    
  \column{0.50\textwidth}

  \pause
  %
  The solution for $x<0$ is
  %
  \begin{equation}
    \psi(x<0) = A e^{\sqrt{z_0^2 - z^2}\, x}, \mathrm{where} \, z_0^2 - z^2 = \frac{2mf}{\hbar^2}
  \end{equation}
  
  and $z_0^2 \equiv 2mV_0/\hbar^2$.

  \pause

  In order to solve for the bound-state energies, we need to solve the following
  %
  \begin{equation}
    \tan(za) = \frac{2 \sqrt{ \left( \frac{z_0}{z} \right)^2 - 1 } }{ 2 - \left( \frac{z_0}{z} \right)^2 }
  \end{equation}
  
  \pause
  
  This is transcendental, requires numerical approaches to finding the \alertbf{``root''} of
  %
  \begin{equation}
    F(z) = \tan(za) - \frac{2 \sqrt{ \left( \frac{z_0}{z} \right)^2 - 1 } }{ 2 - \left( \frac{z_0}{z} \right)^2 }
  \end{equation}
  
  
  
  \end{columns}

\end{frame}

%==========================================================================================
\section[Finding roots]{Finding roots}
%==========================================================================================


%-----------------------------------------------------------------------------------------
\subsection[Bisection method]{Bisection method}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Bisection method}

  The bisection method is straightforward and reliable.
  
  \mysp 
  
  Start with an interval defined by its endpoints, $x_{\ell}$ and $x_{r}$ on the left and right of a starting point, such that
  
  \begin{equation}
    F(x_{\ell})F(x_{r}) < 0
  \end{equation}
  
  That is, the function $F(x)$ has at least one root somewhere between $x_{\ell}$ and $x_{r}$. Then split that interval in half
  
  \begin{equation}
    x_m = \frac{1}{2}(x_{\ell} + x_{r})
  \end{equation}
  
  If the product $F(x_{\ell})F(x_{m}) < 0$, then a root of $F(x)$ lies between $x_{\ell}$ and $x_{m}$, otherwise, it lies between $x_{m}$ and $x_{r}$. Repeat until some tolerance $\epsilon$ is reached such that
  
  \begin{equation}
   | F(x_{m}) | \leq \epsilon
  \end{equation}
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Bisection method}

  The bisection method is straightforward and reliable, \alertbf{but it can be slow if you don't have a good initial guess for the interval}.
  
  \mysp 
  
  If there is no root in the chosen interval, then it does not matter what you do.
  
  \mysp
  
  The method also uses no information whatsoever about the size of the step to take with each iteration. The overall time is determined \alertbf{only} by the size of the initial interval (i.e. how good your \alertbf{guess is}) and the tolerance, $\epsilon$.
  
  \mysp
  
  \pause
  
  \centering \bluebf{We can do better!}
  
\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Newton's method]{Newton's method}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Newton's method (I)}

  The fundamental form of the method alluded to in the last lecture is \bluebf{Newton's method}. The core of this approach is to use information about the function in order to inform how \bluebf{big} of a step size to take with each iteration.
  
  \mysp
  
  Suppose a function $F(x)$ has a root $ x = x_0$ and you are within $\delta$ of that root at $x = x_0 - \delta$. The question we want to answer is: 
  
  \mysp
  
  \begin{ucblock}{}
    How far away from the root are we actually? What is the value of $\delta$?
  \end{ucblock}
  
  \mysp
  
  Using a Taylor expansion (as Wah told us!) 
  
  \begin{equation}
    F(x_0) = F(x+\delta) \approx F(x) + \delta F^{\prime}(x) + \frac{1}{2}\delta^2 F^{\prime\prime}(x) + \mathcal{O}(\delta^3)
  \end{equation}
  
  By setting $F(x_0) = 0$ by assumption, we can make the linear approximation to $\delta \approx \Delta$ as
  
  \begin{equation}
    \Delta = - \frac{F(x)}{F^{\prime}(x)}
  \end{equation}
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Newton's method (II)}

  Therefore, we can choose a starting point $x_i$ and then update $x$ according to Newton
  
  \mysp
  
  \begin{equation}
    x_{i+1} = x_{i} - \frac{F(x_i)}{F^{\prime}(x_i)}
  \end{equation}
  
  The iteration stops after $j$ iterations when 
  
  \begin{equation}
   | x - x_j | \leq \epsilon
  \end{equation}  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Precision of Newton's method}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      For any estimate $x_i$ of the method, the error, $E_i$ is the difference between the true root $x$ and the estimate:
      
      \begin{equation}
        E_i = x - x_i
      \end{equation}
   
      Merely by inspecting the design of Newton's method, you can see that the precision of the estimate for a subsequent iteration will be given by
      %
      \begin{eqnarray}
        E_{i+1} &=& E_i + \frac{F(x)}{F^{\prime}(x)} \\
                &=& -\frac{F^{\prime\prime}(x)}{2F^{\prime}(x)}E_i^2
      \end{eqnarray}
   
    \column{0.5\textwidth}
    
      \begin{figure}
        \centering
        \includegraphics[width=\columnwidth]{NewtonsMethodExample-lnx.png}
      \end{figure}
    
  \end{columns}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Convergence of Newton's method}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      Consequently, Newton's method \bluebf{converges quadratically}
      \begin{itemize}
        \item the error is the square of the error in the previous step)
        \item the number of significant figures is roughly doubled in every iteration, provided that $x_i$ is \alertbf{sufficiently} close to the root.
      \end{itemize} 
         
      However, a critical assumption is that $F^{\prime}(x) \neq 0$; for all $x \in I$, where $I$ is the interval $[x - r, x + r]$ for some $r \geq |x - x_0|$ and $x$ is the true root and $x_0$ was the starting point.
         
    \column{0.5\textwidth}
    
      \begin{figure}
        \includegraphics[width=\columnwidth]{NewtonsMethodExample-lnx.png}
      \end{figure}
    
  \end{columns}

\end{frame}


%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Pathologies and divergent scenarios}

  \setbeamercovered{transparent}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      That is definitely not always the case. Let's look at a pathological example. Here is a fun mystery function that I cooked up (since you need to do something similar on your homework):
      
      \begin{itemize}[<+->]
         \item $x_0 = 2.000$, \bluebf{7 iterations}
         \item $x_0 = -2.000$, \bluebf{2 iterations}
         \item $x_0 = 3.000$, \bluebf{2 iterations}
         \item $x_0 = -1.214$, \bluebf{4 iterations}
         \item Slight modification: $x_0 = -1.213$, \alertbf{no convergence}
         \item Slight modification: $x_0 = 3.000$, \alertbf{no convergence}
      \end{itemize}  
         
    \column{0.5\textwidth}
      
      \begin{figure}
        \centering
        \foreach \n in {1,...,6}%
          {\includegraphics<\n>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology\n.png}}
      \end{figure}
    
  \end{columns}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Backtracking}

  In the last examples above we have a case where the search falls into the pathology of a situation where the initial guess was not \alertbf{sufficiently close} to the root. an ``infinite'' loop without ever getting there. 
  
  \mysp
  
  A solution to this problem is called \alertbf{backtracking}. 
  
  \mysp
  
  \begin{ucblock}{Backtracking}
    In cases where the new guess $x_0 + \Delta x$ leads to an increase in the magnitude of the function, $|f(x_0 + \Delta x)|^{2} > |f(x_0)|^{2}$, you should backtrack somewhat and try a smaller guess, say, $x_0 + \Delta x/2$. If the magnitude of $f$ still increases, then you just need to backtrack some more, say, by trying $x_0 + \Delta x/4$ as your next guess, and so forth.
  \end{ucblock}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Pathological case fixed with backtracking}

  \setbeamercovered{transparent}

  \begin{columns}
  
    \column{0.5\textwidth}
   
      Fixing the pathological example with backtracking:
      
      \begin{itemize}[<+->]
         \item $x_0 = -1.213$, \alertbf{no convergence}
         \item $x_0 = 3.000$, \alertbf{no convergence}
         \item $x_0 = 1.500$, \bluebf{3 iterations}
      \end{itemize}  
         
    \column{0.5\textwidth}
      
      \begin{figure}
        \centering
        \includegraphics<1>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology5.png}
        \includegraphics<2>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology6.png}
        \includegraphics<3>[width=0.95\columnwidth]{NewtonsMethodExample-Pathology7.png}
      \end{figure}
    
  \end{columns}

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[System of equations]{System of equations}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Multidimensional problems}

  Up to this point, we have confined our attention to solving the single equation $F(x) = 0$. Let us now consider the $n$-dimensional version of the same problem, namely
  
  \begin{equation}
    \vec{F}(\vec{x}) = 0
  \end{equation}

  where we allow for a vector of functions $\vec{F} = \{f_1(\vec{x}), f_2(\vec{x}), ..., f_n(\vec{x})\}$, and $\vec{x} = \{ x_1, x_2, ..., x_{n} \}$.
  
  \mysp
  
  The solution of $n$ simultaneous, nonlinear equations is a much more formidable task than finding the root of a single equation. The trouble is the lack of a reliable method for bracketing the solution vector $\vec{x}$. Therefore, we cannot always provide the solution algorithm with a good starting value of $x$, unless such a value is suggested by the physics of the problem.
  
  \mysp
  
  Newton's method is the workhorse here!
  

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Reminder of the general problem}

  Start by considering each one of the $n$ functions, $f_n(x)$, separately:

  \begin{eqnarray}
    f_i(\vec{x}) &=& f_i(\vec{a} )+ \sum_j^n \frac{\partial f_i}{\partial x_j}|_{\vec{x}} \Delta x_j + \frac{1}{2!} \sum_j^n \sum_k^n \frac{\partial^2 f_i}{\partial_j \partial_k}|_{\vec{x}} \Delta x_j \Delta x_k \\
                 %
                 &=& f_i(\vec {a} )+ \Delvec f_i(\vec{a}) \cdot \vec{\Delta x} + \frac{1}{2!}\vec{\Delta x}^{\mathrm {T} }\mathbf {H} (\vec{a})\vec{\Delta x}
  \end{eqnarray}
  
  where $\mathbf {H}$ is the \bluebf{Hessian matrix}, describing the \alertbf{curvature} of $f_i(\vec{x})$ by
  
  \begin{equation}
    {\mathbf {H} }_{j,k}={\frac {\partial ^{2}f(\vec{a})}{\partial x_{j}\partial x_{k}}}
  \end{equation}
  
  (The determinant of $\mathbf {H}$ is also sometimes referred to as \alertbf{the Hessian}) and $\vec{\Delta x}$ is a vector that describes the distance from the point about which we are expanding the function $f_i(\vec{x})$.

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Specific example (I)}

  Let's take a very explicit example of a function:

  \begin{equation}
    f(\vec{x}) = f(x, y) = \ln \sqrt{x^2 + y^2} 
  \end{equation}
  
  and compute the gradient and Hessian matrix at $x=-2, y=1$ (see point on graph below).
    
  \begin{figure}
    \centering
    \includegraphics[width=0.65\columnwidth]{Function1-checkerboard.pdf}
  \end{figure}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Specific example (II)}

  First, we calculate the gradient:
  %
  \begin{eqnarray}
    \partialx{f} &=& \frac{1}{\sqrt{x^2 + y^2}} \left( \frac{1}{2} \frac{2x}{\sqrt{x^2 + y^2}}  \right) \\
                 &=& \frac{x}{x^2 + y^2} \\
    \partialy{f} &=& \frac{y}{x^2 + y^2}
  \end{eqnarray}
  %
  Therefore
  %
  \begin{equation}
    \Delvec f(x,y) = (\frac{x}{x^2 + y^2} , \frac{y}{x^2 + y^2})
  \end{equation}
  %
  Or, in matrix notation
  %
  \begin{equation}
    \Delvec f(x,y) = \left[\begin{array}{c}
                             \frac{x}{x^2 + y^2} \\
                             \frac{y}{x^2 + y^2}
                           \end{array}\right], 
    %                       
    \Delvec f(-2,1) = \left[\begin{array}{c}
                             -0.4 \\
                             0.2
                           \end{array}\right]                     
  \end{equation}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Specific example (III)}

  \begin{figure}
    \centering
    \includegraphics[width=0.85\columnwidth]{Function1-plane.png}
  \end{figure}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Specific example (IV)}

  Next, we calculate the Hessian matrix:
  %
  \begin{eqnarray}
    \partialxsq{f}  &=& \frac{(x^2 + y^2) - x(2x)}{(x^2 + y^2)^2} \\
                    &=& \frac{-x^2 + y^2}{(x^2 + y^2)^2} \\
    \partialysq{f}  &=& \frac{x^2 - y^2}{(x^2 + y^2)^2} \\
    \partialxysq{f} &=& \partialxysq{f} = \frac{-2xy}{(x^2 + y^2)^2}
  \end{eqnarray}
  %
  Therefore
  %
  \begin{equation}
    \mathbf {H}     = \left[\begin{array}{c c}
                             -x^2 + y^2 & -2xy \\
                             -2xy       & x^2 - y^2
                           \end{array}\right] \frac{1}{(x^2 + y^2)^2} , 
    %                       
    \mathbf {H}     = \left[\begin{array}{c c}
                             -0.12 & 0.16 \\
                             0.16  & 0.12
                           \end{array}\right]                    
  \end{equation}

\end{frame}


%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Jacobian matrix}

  In the case that the function $F$ is a vector of functions, $\vec{F} = \{f_1(\vec{x}), f_2(\vec{x}), ..., f_n(\vec{x})\}$ (i.e. a system of equations) then the gradient \alertbf{also has a name:} \bluebf{Jacobian}.
  
  \begin{equation}
    \mathbf {J} = \left[ \partialxone{\vec{f}} \, \cdots \, \partialxn{\vec{f}} \right]%
                = \left[ \begin{array}{c c c}
                             \partialxone{f_1} & \cdots & \partialxn{f_1} \\
                             \vdots            & \ddots & \vdots \\
                             \partialxone{f_n} & \cdots & \partialxn{f_n} 
                           \end{array}\right]
  \end{equation}
  

  \begin{equation}
    \mathbf{J}_{ij}= \partialxj{f_i}
  \end{equation}

  The Jacobian matrix is important because if the function $\vec{f}$ is differentiable at a point $\vec{x}$, then $\mathbf{J}$ defines a linear map which is the best (pointwise) linear approximation of the function $\vec{f}$ near the point $\vec{x}$.
  
  \centering \bluebf{That's exactly what we want!}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Newton's method for a system of equations}

  The following steps constitute Newton's method for simultaneous, nonlinear equations:
  
  \begin{enumerate}
    \item Estimate the solution vector $\vec{x}$
    \item Evaluate $\vec{f}(\vec{x})$
    \item Compute the Jacobian matrix $\mathbf{J}$ ($J_{ij}$)
    \item Setup the simultaneous equations $\mathbf{J}(\vec{x}) \vec{\Delta x} = - \vec{f}(\vec{x})$  and solve for $\vec{x}$
    \item Let $\vec{x}\leftarrow \vec{x} + \vec{\Delta x}$ and repeat steps 2-5
  \end{enumerate}

\end{frame}



%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Comments on matrix computing and manipulations}

  Physical systems are often modeled by systems of simultaneous equations, and it is very convenient to write these matrix form. In fact, several physical systems can be expressed this way very conveniently.
  
  \vspace{0.3cm}
  
  \begin{itemize}
    \item Physical optics (lenses, mirrors, etc)
    \item Electromagnetic waves propagating near boundaries and in matter
    \item Quantum mechanical systems
    \item Classical mechanical dynamical systems
    \item etc, etc
  \end{itemize}
  
  As the models are made more realistic, the matrices often become large, and comput- ers become an excellent tool for solving such problems.

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Example: weights and strings (I)}

  \begin{figure}
    \centering
    \includegraphics[width=0.85\columnwidth]{WeightsStrings.pdf}
  \end{figure}
  
  Two weights $(W_1, W_2) = (10, 20)$ are hung from three pieces of string with lengths $(L_1, L_2, L_3) = (3, 4, 4)$ and a horizontal bar of length $L = 8$. 
  
  \mysp
  
  We want to find the angles assumed by the strings and the tensions exerted by the strings.
 
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Example: weights and strings (II)}

  The first 5 equations implement the geometric constraints that the horizontal length of the structure is $L$ and that the strings begin and end at the same height.
  %
  \begin{eqnarray}
    L_1 \cos\theta_1 + L_2 \cos\theta_2 + L_3 \cos\theta_3 &=& L \\
    L_1 \sin\theta_1 + L_2 \sin\theta_2 - L_3 \sin\theta_3 &=& 0 \\
    \sin^2 \theta_1 + \cos^2 \theta_1                      &=& 1 \\
    \sin^2 \theta_2 + \cos^2 \theta_2                      &=& 1 \\
    \sin^2 \theta_3 + \cos^2 \theta_3                      &=& 1
  \end{eqnarray}
  %
  The last three equations include trigonometric identities as independent equations because we are treating $\sin\theta$ and $\cos\theta$ as independent variables; this makes the search procedure easier to implement.
  
\end{frame}


%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Example: weights and strings (III)}

  The basics physics says that since there are no accelerations, the sum of the forces in the horizontal and vertical directions must equal zero.
  %
  \begin{eqnarray}
    T_1 \sin\theta_1 - T_2 \sin\theta_2 - W_1              &=& 0 \\
    T_1 \cos\theta_1 - T_2 \cos\theta_2                    &=& 0 \\
    T_2 \sin\theta_2 + T_3 \sin\theta_3 - W_2              &=& 0 \\
    T_2 \cos\theta_2 - T_3 \cos\theta_3                    &=& 0  
  \end{eqnarray}

  Here $W_i$ is the weight of mass $i$ and $T_i$ is the tension in string $i$. Note that since we do not have
a rigid structure, we cannot assume the equilibrium of torques.
  
  \mysp
  
  These equations represent nine simultaneous nonlinear equations. While linear equations can be solved directly, nonlinear equations cannot.

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Example: weights and strings (IV)}

  We apply to our set the same Newton's method algorithm as used to solve a single equation by renaming the nine unknown angles and tensions as the components of our vector $\vec{x}$ and placing the variables together as a vector:
  %
  \begin{equation}
    \vec{x} = \left[\begin{array}{c}
                      x_1 \\
                      x_2 \\
                      x_3 \\
                      x_4 \\
                      x_5 \\
                      x_6 \\
                      x_7 \\
                      x_8 \\
                      x_9        
                    \end{array}\right]
    %                       
             = \left[\begin{array}{c}
                       \sin\theta_1 \\
                       \sin\theta_2 \\
                       \sin\theta_3 \\
                       \cos\theta_1 \\
                       \cos\theta_2 \\
                       \cos\theta_3 \\
                       T_1 \\
                       T_2 \\
                       T_3
                     \end{array}\right]  \nonumber                   
  \end{equation}

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Example: weights and strings (V)}

  The nine equations to be solved are written in a general form with zeros on the right-hand sides and placed in a vector:
  %
  \begin{equation}
    \vec{f(\vec{x}}) = \left[\begin{array}{c}
                      f_1(\vec{x}) \\
                      f_2(\vec{x}) \\
                      f_3(\vec{x}) \\
                      f_4(\vec{x}) \\
                      f_5(\vec{x}) \\
                      f_6(\vec{x}) \\
                      f_7(\vec{x}) \\
                      f_8(\vec{x}) \\
                      f_9(\vec{x})        
                    \end{array}\right]
    %                       
             = \left[\begin{array}{c}
                       L_1 x_4 + L_2 x_5 + L_3 x_6 - 8 \\
                       L_1 x_1 + L_2 x_2 - L_3 x_3 \\
                       x_7 x_1 - x_8 x_2 - W_1 \\
                       x_7 x_4 - x_8 x_5 \\
                       x_8 x_2 + x_9 x_3 - W_2\\
                       x_8 x_5 - x_9 x_6 \\
                       x_1^2 + x_4^2 - 1 \\
                       x_2^2 + x_5^2 - 1 \\
                       x_3^2 + x_6^2 
                     \end{array}\right]  
    %                       
             = \left[\begin{array}{c}
                       3x_4 + 4x_5 +4x_6 - 8 \\
                       3x_1 + 4x_2 - 4x_3 \\
                       x_7 x_1 - x_8 x_2 - 10 \\
                       x_7 x_4 - x_8 x_5 \\
                       x_8 x_2 + x_9 x_3 - 20 \\
                       x_8 x_5 - x_9 x_6 \\
                       x_1^2 + x_4^2 - 1 \\
                       x_2^2 + x_5^2 - 1 \\
                       x_3^2 + x_6^2 
                     \end{array}\right] \nonumber                 
  \end{equation}

  And this is then solved by computing the Jacobian and using Newton's method as usual!

\end{frame}


%==========================================================================================
%==========================================================================================
\end{document}
