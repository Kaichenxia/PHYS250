%----------------------------------------------------
% Setup Beamer
%----------------------------------------------------
\documentclass[hyperref={colorlinks=true}]{beamer}

%----------------------------------------------------
% Packages to use
%----------------------------------------------------
\input{../packages.sty}

%----------------------------------------------------
% Setup Theme
%----------------------------------------------------
\input{../theme.sty}

%----------------------------------------------------
% Table of Contents at each section transition
%----------------------------------------------------

\AtBeginSection[]
{
   \begin{frame}
       \frametitle{Outline}
       \setcounter{tocdepth}{2}
       \tableofcontents[currentsection]
   \end{frame}
}

%----------------------------------------------------
% Colors
%----------------------------------------------------
\input{../mycolors.sty}

%----------------------------------------------------
% Style, formatting, and new commands
%----------------------------------------------------
\input{../../global.sty}
\input{../newcommands.sty}
\input{../EandMcommands.sty}

%----------------------------------------------------
% Set paths for plots and images
%----------------------------------------------------
\input{../paths.sty}

%----------------------------------------------------
% SETTINGS FOR THIS LECTURE
%----------------------------------------------------
\newcommand{\lecnum }  {Lecture 13}
<<<<<<< HEAD
\newcommand{\lecdate}  {November 19, 2019}
\newcommand{\topic}    {Neural Networks -- Part I}
=======
\newcommand{\lecdate}  {November 15, 2019}
\newcommand{\topic}    {Fourier Transforms!}
>>>>>>> 7f824f9866943a6bee9ebc3489e7b00256b4f4f1

%-----------------------------------------------------------------------------------------
% Title: [Column]{Title}
%-----------------------------------------------------------------------------------------
\title[PHYS 250 (Autumn 2019) -- \lecnum]{\topic}

%-----------------------------------------------------------------------------------------
% SubTitle: [Column]{Subtitle}
%-----------------------------------------------------------------------------------------
\subtitle{PHYS 250 (Autumn 2019) -- \lecnum}

%-----------------------------------------------------------------------------------------
% Author: [SubAuthor]{Author}
%-----------------------------------------------------------------------------------------
\author[D.W.~Miller]{David Miller}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\institute[EFI, Chicago] 
{
  Department of Physics and the Enrico Fermi Institute\\
  University of Chicago
}

%----------------------------------------------------
% Institute: [SubInst]{Institute}
%----------------------------------------------------
\date[\lecdate]{\lecdate}

\subject{PHYS 250 Lecture}

\begin{document}

%==========================================================================================
% TITLE PAGE
%==========================================================================================

{
\begin{frame}
  \titlepage
\end{frame}
}

%==========================================================================================
<<<<<<< HEAD
\section[Introduction to Neural Networks]{Introduction to Neural Networks}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Context of Machine Learning]{Context of Machine Learning}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Machine learning generally}

  Machine learning is an area of study based broadly on developments in computer science over the past 50 years that aims to use \bluebf{algorithmic approaches to discovering, identifying, and analyzing, patterns of interest in datasets}.
  
  \begin{itemize}
    \item Key phrase here is \bluebf{algorithmic}
    \item We want \bluebf{models} that tell us something about a dataset
  \end{itemize}
   
  \begin{center} \Large \alertbf{Sounds like something we'd be interested in!} \end{center}
  
  \ra\ Definitely, but much broader concept here than just what we've been discussing so far in the course.

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Machine learning applications}

  \setbeamercovered{transparent}

  What are we using these algorithms to \textit{do}?
  
  \mysp
  
  \begin{itemize}[<+->]
    \item \textbf{Supervised learning:} \bluebf{machine is presented examples of multiple classes and learns to differentiate} (\textit{discover known patterns on unknown data})
    \begin{itemize}
      \item \alertbf{Classification:} identify categories of variables; e.g. signal vs. background
      \item \alertbf{Regression:} estimate relationships among variables; e.g. calibration
      \item \alertbf{Generation:} generate a set of variables from a category (e.g. image)
    \end{itemize}
    \item \textbf{Unsupervised learning:} \bluebf{machine is presented data and asked to provide multiple classes} (\textit{discover unknown patterns on known data})
    \begin{itemize}
      \item \alertbf{Clustering:} grouping set of objects in a dataset; e.g. jets of hadrons
      \item \alertbf{Anomaly detection:} identifying rare events or observations; e.g. errors
    \end{itemize}
  \end{itemize}
  
  \begin{figure}
    \centering
    \includegraphics<1-2>[width=0.4\textwidth]{Classification.png}
    \includegraphics<3>[width=0.4\textwidth]{Regression.png}
    \includegraphics<4>[width=0.4\textwidth]{Generation.jpg}
    \includegraphics<5-6>[width=0.4\textwidth]{Clustering.png}
    \includegraphics<7>[width=0.3\textwidth]{Anomaly.png}
  \end{figure}


\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Where do neural networks fit in?}

  There are many \bluebf{tools} that we can use to \alertbf{learn} how to perform these tasks. 
  
  \mysp

  \begin{ucblock}{Neural networks}
  Neural networks provide a framework for non-linear fitting and parameter estimation with limited input data but for cases in which the input data are understood and the categories (for example) are known.  
  \end{ucblock}
  
  \mysp
    
  \begin{center} \bluebf{Neural networks are \underline{proven} to be ``universal approximators''} \end{center}
  
  See \href{https://doi.org/10.1016/0893-6080(91)90009-T}{Kurt Hornik, ``Approximation Capabilities of Multilayer Feedforward Networks'', Neural Networks, 4, 251-257 (1991)}.
  
  \mysp
  
  One straightforward way to think of this is as a composition of functions $f(\mathbb{A}\mathbf{x}+\mathbf{b})$ for inputs $\mathbf{x}$ (features) matrix $\mathbb{A}$ (weights), bias $\mathbf{b}$, non-linearity $f$. Given some data, the algorithm should tell us which of a pre-defined set of categories the data fall into, even if it's a complicated non-linear function that describes the boundary between these categories.

  
\end{frame}
  
%-----------------------------------------------------------------------------------------
\subsection[Motivations from the perspective of modeling]{Motivations from the perspective of modeling}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Motivations for neural networks}

  If we can use a neural network to approximate any (incl. non-linear) function, then this is strong motivation for using it to learn complicated functions that describe classes of objects, potentially in high-dimensional spaces.

  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{SimpleNeuralNetwork.pdf}
  \end{figure}
  
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Very, very naive strawman example}

  Suppose the ``function'' we want to learn is to map kilometers\ra\ miles. 
  
  \mysp
  
  Given a \alertbf{``training dataset''} of input and output pairs, we can test a simple function with a parameter the defines the conversion, and then \bluebf{iteratively update} the parameters based on how far from the \alertbf{``true''} value we end up.

  \begin{figure}
    \centering
    \includegraphics<1>[width=0.7\textwidth]{km-to-mi-1.png}%
    \includegraphics<2>[width=0.8\textwidth]{km-to-mi-2.png}%
    \includegraphics<3>[width=0.8\textwidth]{km-to-mi-3.png}%
    \includegraphics<4>[width=0.8\textwidth]{km-to-mi-4.png}%
    \includegraphics<5>[width=0.8\textwidth]{km-to-mi-5.png}%
\end{figure}
  
  Obviously, we want to extend this to non-linear cases.
  
\end{frame}


%-----------------------------------------------------------------------------------------
\subsection[Motivations from the perspective of biology]{Motivations from the perspective of biology}
%-----------------------------------------------------------------------------------------

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{The biological model}

  The way brains work is by establishing connections between various neurons.
  
  \mysp
  
  Once connections are made, initial stimuli give similar (though not always identical) results
  
  \begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{NeuronModel.pdf}%
\end{figure}
    
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Artificial neuron model}

  McCulloch and Pitts (1943) model of a single neuron. Rosenblatt (1958) then developed the \bluebf{ perceptron} based on this, the oldest neural network still in use today; provided an algorithm to train the perceptron network, built it in electronics, and proved convergence in linearly separable case.
  
  \mysp
    
  \begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{The-McCulloch-and-Pitts-1943-model-of-a-single-neuron-Source-Adapted-from-Franck.png}%
    \includegraphics[width=0.45\textwidth]{PittsModel.pdf}%
\end{figure}
    
\end{frame}



%-----------------------------------------------------------------------------------------
\subsection[Discussion of applications]{Discussion of applications}
%-----------------------------------------------------------------------------------------

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Neural networks are, maybe literally, everywhere}

    
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.95\textwidth]{NNs-1.png}%
    \includegraphics<2>[width=0.95\textwidth]{NNs-2.png}%
    \includegraphics<3>[width=0.95\textwidth]{NNs-3.png}%
    \includegraphics<4>[width=0.95\textwidth]{QuarksGluons.pdf}%
\end{figure}
    
\end{frame}


%==========================================================================================
\section[Foundations of Neural Networks]{Foundations of Neural Networks}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Activation functions]{Activation functions}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Activation functions}

  We need a network that can effectively learn a complex function by weighting the individual outputs of each node.  
    
  \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{NN-2layer.png}%
  \end{figure}
    
\end{frame}




%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Non-linearity functions for learning}

  This is the function that defines the output of a node. Typically, we want to map the resulting values into a range between $[0, 1]$, or  $[-1,1]$
    
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.85\textwidth]{NonLinearFunction.pdf}%
    \includegraphics<2>[width=0.85\textwidth]{NonLinearFunction-2.pdf}%
  \end{figure}
    
\end{frame}


%-----------------------------------------------------------------------------------------
\subsection[Training]{Training}
%-----------------------------------------------------------------------------------------

%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Training a neural network}

  The \bluebf{``objective function''} is minimized using \alertbf{``stochastic gradient decent''}:
  \begin{itemize}
    \item iterative method for optimizing the ``loss'' or the error that we are trying to minimize 
  \end{itemize}
    
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.75\textwidth]{Training.pdf} \\
  \end{figure}
    
=======
\section[Reminders]{Reminders}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Reminders from Lecture 12]{Reminders from Lecture 12}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Reminders from last time}

  We discussed both Poisson's equation and Fourier Series in the last lecture.
  
  \vspace{0.3cm}
  
  \begin{ucblock}{PDEs and Fourier Series}
    \begin{itemize}
      \item \bluebf{PDEs: Poisson's Equation} 
      \begin{itemize}
        \item We discussed and then wrote a function for computing the solution to Poisson's equation
        \item Specifically, we elaborated on how we could structure the (relatively simple) function in order to take various constraints (i.e. sources) for the solution into account more easliy
      \end{itemize}
      \item \bluebf{Fourier Series:} 
      \begin{itemize}
        \item Started discussing the basics of Fourier Series 
        \item Evaluated, computationally, the coefficients of a simple series for both a square and sawtooth wave
      \end{itemize}
    \end{itemize}
  \end{ucblock}
  
  \mysp
  
  Today we will go much more in depth with Fourier Transforms and Analysis! This will be a mixture of Python Notebooks and Lecture Slides

\end{frame}


%==========================================================================================
\section[Follow-up with the Square Wave]{Follow-up with the Square Wave}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Square Wave]{Square Wave}
%-----------------------------------------------------------------------------------------

\begin{frame}%[shrink=10]
  \frametitle{Square Wave}

  First, I want to do a little follow-up with the Fourier Series for the \alertbf{square wave} function that we discussed last time.
  
  Recall that we determined the coefficients of a sine and cosine expansion to be:
  
  \begin{equation}
    a_n =\frac{2}{n\pi}\sin \left(n \omega_0 \frac{\pi}{2} \right)
  \end{equation}
  
  which yields a discrete function:
  
  \begin{equation}
    f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} a_n \cos(nx) 
  \end{equation}
  
  \centering \bluebf{Now open up the \texttt{Fourier-Transforms-Analysis.ipynb} jupyter notebook so that we can look at this in more detail!}

\end{frame}

%==========================================================================================
\section[Extension to the Fourier Transform]{Extension to the Fourier Transform}
%==========================================================================================

%-----------------------------------------------------------------------------------------
\subsection[Euler's Formula]{Euler's Formula}
%-----------------------------------------------------------------------------------------


\begin{frame}%[fragile]
  \frametitle{Euler's Formula}

  We can generalize this by making use of Euler's Formula.
  
  \begin{columns}
  
  \column{0.5\textwidth}
  
  Euler's formula states that for any real number $\phi$: 
  
  \begin{equation}
    e^{i\phi} = \cos(\phi) + i\sin(\phi)
  \end{equation}
  
  When $\phi=\pi$, Euler's formula evaluates to 
  
  \begin{equation}
    e^{i\pi }+1=0,
  \end{equation} 
  
  which is known as Euler's identity. 
  
  \column{0.5\textwidth}
  
  \begin{figure}
    \includegraphics[width=0.9\columnwidth]{Eulers_formula.png}
  \end{figure}
  
  \end{columns}
  
  The implication is that it is possible to recover the amplitude of each wave in a Fourier series using an integral, which has many useful properties (in particular, that it's then continuous).

\end{frame}

%-----------------------------------------------------------------------------------------
\subsection[Fourier Transforms]{Fourier Transforms}
%-----------------------------------------------------------------------------------------

\begin{frame}%[fragile,shrink=15]
  \frametitle{Fourier Transforms (I)}

  I will use the following definitions for the Fourier transform $\hat{f}(\xi)$ of a function $f(x)$, where $x$ typically represents either a \bluebf{spatial or time domain}, and $\xi$ typically represents a corresponding inverse notion of \alertbf{spatial or time frequency}.

  \begin{equation}
    \hat{f}(\xi)=\int_{-\infty}^{\infty} f(x) e^{-2\pi i \xi x } dx
  \end{equation}
  
  The \alertbf{inverse transform} is then obtained via
  
  \begin{equation}
    f(x)=\int_{-\infty}^{\infty} \hat{f}(\xi) e^{2\pi i \xi x } d\xi 
  \end{equation}

  In the case of spatial coordinates, $x$ denotes length and $\xi$ denotes inverse wavelength: $\xi = \frac{1}{\lambda}$. In the time domain, $x$ denotes time and $\xi$ denotes frequency. In the case that $x=t$ is in seconds, but $\xi$ is \alertbf{angular} frequency $\omega$ then a factor of $2\pi$ appears to get the normalization correct.

\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[fragile,shrink=15]
  \frametitle{Fourier Transforms (II)}

  In the case of spatial coordinates, $x$ denotes length and $\xi$ denotes inverse wavelength: $\xi = \frac{1}{\lambda}$. In the time domain, $x$ denotes time and $\xi$ denotes frequency. In the case that $x=t$ is in seconds, but $\xi$ is \alertbf{angular} frequency $\omega$ then a factor of $2\pi$ appears to get the normalization correct.
  
  \begin{eqnarray}
    \hat{f}(\omega) &=& \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f(t) e^{-i\omega t} dt \\
    f(t)            &=& \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \hat{f}(\omega) e^{i \omega t } d\omega
  \end{eqnarray}

  Since $\omega = 2\pi \xi = \frac{2\pi}{\lambda}$.
  
  \mysp
  
  The $\frac{1}{\sqrt{2\pi}}$ factor in both these integrals is a common normalization in quantum mechanics but maybe not in engineering where only a single $\frac{1}{2\pi}$ factor is often used.
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[fragile,shrink=15]
  \frametitle{Discrete Fourier Transforms (I)}

  If $\hat{f}(\omega)$ or $f(t)$ are known analytically or numerically, the Fourier transform integrals can be evaluated using the integration techniques studied earlier. 
  
  \mysp \pause
  
  In practice, the signal $f(t)$  is measured, or \alertbf{sampled} at just a finite number $N$ of times $t$, and these are what we must use to approximate the transform. 
  
  \mysp \pause
  
  The resultant \alertbf{discrete Fourier transform (DFT)} is an approximation both because the signal is not known for all times and because we integrate numerically.
  
  \mysp \pause
  
  Once we have a discrete set of transforms, they can be used to reconstruct the signal for any value of the time. 
  
  \mysp \pause
  
  In this way the \bluebf{DFT can be thought of as a technique for interpolating, compressing, and extrapolating data}.
  
\end{frame}

%-----------------------------------------------------------------------------------------

\begin{frame}%[fragile,shrink=15]
  \frametitle{Discussion}

  \centering \Large \bluebf{Do you see any issues with this ``sampling''? }
  
  \pause
  
  \begin{figure}
    \includegraphics[width=0.9\columnwidth]{Aliasing.pdf}
  \end{figure}
  
>>>>>>> 7f824f9866943a6bee9ebc3489e7b00256b4f4f1
\end{frame}

%-----------------------------------------------------------------------------------------

<<<<<<< HEAD
\begin{frame}%[shrink=10]
  \frametitle{Back-propagation}

  For the matrix that defines the \alertbf{weights} we want to update those weights based on 
    
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.85\textwidth]{Learning.pdf} 
  \end{figure}
    
  Here, \bluebf{back-propagation} is shorthand for ``the backward propagation of errors'' since an error is computed at the output and distributed backwards throughout the network's layers.
  
  \mysp
  
  Really, \bluebf{back-propagation} is \textbf{just a special case of computational differentiation!}. 
    
=======
\begin{frame}%[fragile,shrink=15]
  \frametitle{Discrete Fourier Transforms (II)}

  The DFT algorithm results from evaluating the integral not from $1$ to $+1$ but rather from time $0$ to time $T$ over which the signal is measured, and from approximating the integration of the integral by computing a discrete sum:
  
  \begin{eqnarray}
    \hat{f}(\omega_n) &=& \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f(t)   e^{-i\omega_n t}   dt \\
                 &\simeq& \frac{1}{\sqrt{2\pi}} \int_{0}^{T}            f(t)   e^{-i\omega_n t}   dt \\
                 &\simeq& \frac{1}{\sqrt{2\pi}} \sum_{k=1}^{N}        h f(t_k) e^{-i\omega_n t_k} \qquad (h\equiv\mathrm{stepsize}) \\
                 &\simeq& \frac{h}{\sqrt{2\pi}} \sum_{k=1}^{N}          f_k    e^{-2\pi i k n/N}     \\
    \hat{f}_n \equiv \frac{\hat{f}(\omega_n)}{h} &=& \frac{1}{\sqrt{2\pi}} \sum_{k=1}^{N}  f_k  e^{-2\pi i k n/N}  
  \end{eqnarray}
  
>>>>>>> 7f824f9866943a6bee9ebc3489e7b00256b4f4f1
\end{frame}

%-----------------------------------------------------------------------------------------

<<<<<<< HEAD
\begin{frame}%[fragile, shrink=20]
  \frametitle{Back-propagation}

  Phase 1: propagation
  \begin{itemize}
    \item Propagation forward through the network to generate the output value(s)
    \item Calculation of the cost (error term)
    \item Propagation of the output activations back through the network using the training pattern target to generate the deltas (the difference between the targeted and actual output values) of all output and hidden neurons.
  \end{itemize}

  Phase 2: weight update

  \begin{itemize}
    \item The weight's output delta and input activation are multiplied to find the gradient of the weight.
    \item A ratio (percentage) of the weight's gradient is subtracted from the weight.
  \end{itemize}

This ratio (percentage) influences the speed and quality of learning; it is called the learning rate ($\eta$ on the previous slide). The greater the ratio, the faster the neuron trains, but the lower the ratio, the more accurate the training is. The sign of the gradient of a weight indicates whether the error varies directly with, or inversely to, the weight. Therefore, the weight must be updated in the opposite direction, ``descending'' the gradient. 

  \begin{figure}
    \centering
    \includegraphics<1>[width=0.99\textwidth]{Pseudocode.png} 
  \end{figure}
 
  
    
=======
\begin{frame}%[fragile,shrink=15]
  \frametitle{Discrete Fourier Transforms (III)}

  We then need the inverse as well, which we can obtain with $d\omega \rightarrow 2\pi/Nh $ we invert the $\hat{f}_n $
      
  \begin{eqnarray}
    f_k &=& \frac{1}{\sqrt{2\pi}} \sum_{n=1}^{N} \frac{2\pi}{Nh}  \hat{f}_n  e^{i\omega_n t}  
  \end{eqnarray}
  
  Once we know the $N$ values of the transform $\hat{f}_n$, we can use this expression to evaluate $f(t)$ for any time $t$. The frequencies $\omega n$ are determined by the number of samples taken and by the total sampling time $T = N h$ as
  
  \begin{equation}
    \omega_n = n\frac{2\pi}{Nh}
  \end{equation}
  
  Clearly, the larger we make the time $T = Nh$ over which we sample the function, the smaller will be the frequency steps or resolution. Accordingly,if you want a smooth frequency spectrum, you need to have a small frequency step $2\pi/T$.
  
>>>>>>> 7f824f9866943a6bee9ebc3489e7b00256b4f4f1
\end{frame}

%-----------------------------------------------------------------------------------------

<<<<<<< HEAD
\begin{frame}%[fragile, shrink=20]
  \frametitle{Training and validation}

  \begin{figure}
    \centering
    \includegraphics<1>[width=0.99\textwidth]{Validation.pdf} 
  \end{figure}
 
  
    
=======
\begin{frame}%[fragile,shrink=15]
  \frametitle{Discrete Fourier Transforms (IV)}

  Lastly, we can simplify this expression to yield a clear computational approach:
      
  \begin{eqnarray}
    f_k       &=& \frac{\sqrt{2\pi}}{N} \sum_{n=1}^{N} Z^{-nk}\hat{f}_{n} \qquad (Z=e^{-2\pi i/N}) \\
    \hat{f}_n &=& \frac{1}{\sqrt{2\pi}} \sum_{k=1}^{N} Z^{nk} f_k \qquad (n=0,1,\cdots,N) 
  \end{eqnarray}
  
  With this formulation, the computer needs to compute only powers of $Z$.
  
>>>>>>> 7f824f9866943a6bee9ebc3489e7b00256b4f4f1
\end{frame}

%==========================================================================================
%==========================================================================================
\end{document}
